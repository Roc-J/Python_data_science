{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "朴素贝叶斯算法\n",
    "\n",
    "使用Python自带的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "应用自带的库来实现数据的获取，自带的数据中需要抽取分类，对于每个分类下的文件，获取属于这个分类的词语\n",
    "\n",
    "数据格式返回的是列表的元组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    dataset = []\n",
    "    y_labels = []\n",
    "    # 抽取分类\n",
    "    for cat in movie_reviews.categories():\n",
    "        # 对每个类别下的文件\n",
    "        for fileid in movie_reviews.fileids(cat):\n",
    "            # 获取属于这个分类的词语\n",
    "            words = list(movie_reviews.words(fileid))\n",
    "            dataset.append((words,cat))\n",
    "            y_labels.append(cat)\n",
    "    return dataset,y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_test(input_dataset,ylabels):\n",
    "    '''\n",
    "    将数据分为训练数据和测试数据\n",
    "    '''\n",
    "    train_size = 0.7\n",
    "    test_size = 1 - train_size\n",
    "    \n",
    "    stratified_split = StratifiedShuffleSplit(ylabels,test_size=test_size,n_iter=1,random_state=77)\n",
    "    \n",
    "    for train_indx,test_indx in stratified_split:\n",
    "        train = [input_dataset[i] for i in train_indx]\n",
    "        train_y = [ylabels[i] for i in train_indx]\n",
    "        \n",
    "        test = [input_dataset[i] for i in test_indx]\n",
    "        test_y = [ylabels[i] for i in test_indx]\n",
    "        \n",
    "    return train,test,train_y,test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面介绍3个函数，主要是特征生成函数：为了分类，需要提供特征或者属性，给定一个评价，这些函数能从中生成一系列特征。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_word_features(instance):\n",
    "    # 用字典来保存特征\n",
    "    feature_set = {}\n",
    "    # 词列表的实例元组里的第一个子项\n",
    "    words = instance[0]\n",
    "    for word in words:\n",
    "        feature_set[word] = 1 \n",
    "    return (feature_set,instance[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_negate_features(instance):\n",
    "    '''\n",
    "    if a word is preceeded by either 'not' or 'no'\n",
    "    this function adds a prefix 'Not_' to that word\n",
    "    It will also not insert the previous negation word\n",
    "    'not' or 'no' in feature dictionary\n",
    "    '''\n",
    "    # 对词进行检索，即实例元组中的第1个子项\n",
    "    words = instance[0]\n",
    "    final_words = []\n",
    "    # 用一个布尔变量追踪上一个词是不是负面词\n",
    "    negate = False\n",
    "    negate_words = ['no','not']\n",
    "    for word in words:\n",
    "        if negate:\n",
    "            word = 'Not_' + word\n",
    "            negate = False\n",
    "        if word not in negate_words:\n",
    "            final_words.append(word)\n",
    "        else:\n",
    "            negate =True\n",
    "    feature_set = {}\n",
    "    for word in final_words:\n",
    "        feature_set[word] = 1\n",
    "    return (feature_set,instance[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(in_data):\n",
    "    stopword_list = stopwords.words('english')\n",
    "    negate_words = ['no','not']\n",
    "    new_stopwords = [word for word in stopword_list if word not in negate_words]\n",
    "    label = in_data[1]\n",
    "    # 删除停用词\n",
    "    words = [word for word in in_data[0] if word not in new_stopwords]\n",
    "    return (words,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_keyphrase_features(instance):\n",
    "    feature_set = {}\n",
    "    instance = remove_stop_words(instance)\n",
    "    words = instance[0]\n",
    "    # 采用二元特征原始频率计数\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    # 二元特征按出现的频率降序排序，选择前400\n",
    "    bigrams = bigram_finder.nbest(BigramAssocMeasures.raw_freq,400)\n",
    "    for bigram in bigrams:\n",
    "        feature_set[bigram] = 1\n",
    "    return (feature_set,instance[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(features):\n",
    "    model = nltk.NaiveBayesClassifier.train(features)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def probe_model(model,features,dataset_type='Train'):\n",
    "    accuracy = nltk.classify.accuracy(model,features)\n",
    "    print '\\n' + dataset_type + 'Accuracy = %0.2f'%(accuracy*100) + '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_features(model,no_features=5):\n",
    "    print '\\nFeature Importance'\n",
    "    print '==================='\n",
    "    print model.show_most_informative_features(no_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第1次就得到正确的模型是非常困难的，需要反复尝试不同的特征和参数调优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model_cycle_1(train_data,dev_data):\n",
    "    # 为训练集建立特征\n",
    "    train_features = map(build_word_features,train_data)\n",
    "    # 为测试集建立特征\n",
    "    dev_features = map(build_word_features,dev_data)\n",
    "    # 建模\n",
    "    model = build_model(train_features)\n",
    "    # 模型探测\n",
    "    probe_model(model,train_features)\n",
    "    probe_model(model,dev_features,'Dev')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model_cycle_2(train_data,dev_data):\n",
    "    # 为训练集建立特征\n",
    "    train_features = map(build_negate_features,train_data)\n",
    "    # 为测试集建立特征\n",
    "    dev_features = map(build_negate_features,dev_data)\n",
    "    # 建模\n",
    "    model = build_model(train_features)\n",
    "    # 模型探测\n",
    "    probe_model(model,train_features)\n",
    "    probe_model(model,dev_features,'Dev')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model_cycle_3(train_data,dev_data,test_data):\n",
    "    # 为训练集建立特征\n",
    "    train_features = map(build_keyphrase_features,train_data)\n",
    "    # 为测试集建立特征\n",
    "    dev_features = map(build_keyphrase_features,dev_data)\n",
    "    # 建模\n",
    "    model = build_model(train_features)\n",
    "    # 模型探测\n",
    "    probe_model(model,train_features)\n",
    "    probe_model(model,dev_features,'Dev')\n",
    "    test_features = map(build_keyphrase_features,test_data)\n",
    "    probe_model(model,test_features,'Test')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original Data size = 2000\n",
      "\n",
      "Training Data size = 1399\n",
      "\n",
      "Dev Data size =  420\n",
      "\n",
      "Test Data size =  181\n",
      "\n",
      "TrainAccuracy = 97.57%\n",
      "\n",
      "DevAccuracy = 68.57%\n",
      "\n",
      "Feature Importance\n",
      "===================\n",
      "Most Informative Features\n",
      "               stupidity = 1                 neg : pos    =     15.6 : 1.0\n",
      "                  warned = 1                 neg : pos    =     11.7 : 1.0\n",
      "             wonderfully = 1                 pos : neg    =     11.5 : 1.0\n",
      "             outstanding = 1                 pos : neg    =     11.0 : 1.0\n",
      "            unconvincing = 1                 neg : pos    =     11.0 : 1.0\n",
      "None\n",
      "\n",
      "TrainAccuracy = 98.00%\n",
      "\n",
      "DevAccuracy = 69.29%\n",
      "\n",
      "Feature Importance\n",
      "===================\n",
      "Most Informative Features\n",
      "               stupidity = 1                 neg : pos    =     15.6 : 1.0\n",
      "             wonderfully = 1                 pos : neg    =     14.7 : 1.0\n",
      "               Not_funny = 1                 neg : pos    =     13.0 : 1.0\n",
      "                  warned = 1                 neg : pos    =     11.7 : 1.0\n",
      "             outstanding = 1                 pos : neg    =     11.0 : 1.0\n",
      "None\n",
      "\n",
      "TrainAccuracy = 100.00%\n",
      "\n",
      "DevAccuracy = 82.86%\n",
      "\n",
      "TestAccuracy = 76.80%\n",
      "\n",
      "Feature Importance\n",
      "===================\n",
      "Most Informative Features\n",
      "     (u'waste', u'time') = 1                 neg : pos    =     13.0 : 1.0\n",
      "      (u'one', u'worst') = 1                 neg : pos    =     13.0 : 1.0\n",
      "        (u'-', u'notch') = 1                 pos : neg    =     11.7 : 1.0\n",
      "      (u'perfect', u',') = 1                 pos : neg    =     11.7 : 1.0\n",
      "      (u'.', u'cameron') = 1                 pos : neg    =     11.7 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    input_dataset,y_labels = get_data()\n",
    "    # 训练数据\n",
    "    train_data,all_test_data,train_y,all_test_y = get_train_test(input_dataset,y_labels)\n",
    "    # DEV数据\n",
    "    dev_data,test_data,dev_y,test_y = get_train_test(all_test_data,all_test_y)\n",
    "    \n",
    "    # 查看不同数据集的大小\n",
    "    print '\\noriginal Data size =',len(input_dataset)\n",
    "    print '\\nTraining Data size =',len(train_data)\n",
    "    print '\\nDev Data size = ',len(dev_data)\n",
    "    print '\\nTest Data size = ',len(test_data)\n",
    "    \n",
    "    # 建模的不同过程\n",
    "    model_cycle_1 = build_model_cycle_1(train_data,dev_data)\n",
    "    show_features(model_cycle_1)\n",
    "    \n",
    "    model_cycle_2 = build_model_cycle_2(train_data,dev_data)\n",
    "    show_features(model_cycle_2)\n",
    "    \n",
    "    model_cycle_3 = build_model_cycle_3(train_data,dev_data,test_data)\n",
    "    show_features(model_cycle_3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
